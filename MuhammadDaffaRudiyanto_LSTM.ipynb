{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NRemyC3V2cJ"
      },
      "outputs": [],
      "source": [
        "#Import all neccesary libraries\n",
        "%reset -f\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout, Input, Normalization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score, f1_score\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read and store the CSV file into a variable\n",
        "data = pd.read_csv('dataset_elec_4000.csv') \n",
        "data = data[['review','rating']] \n",
        "\n",
        "# Set a variable for positive reviews counter\n",
        "positive = 0 \n",
        "\n",
        "# Set a variable for negative reviews counter\n",
        "negative = 0 \n",
        "\n",
        "for i in range(len(data['rating'])):\n",
        "  # Counts the amount of positive reviews in the data set\n",
        "  if data['rating'][i] == 1.0:\n",
        "    positive += 1 \n",
        "  # Counts the amount of negative reviews in the data set\n",
        "  else:\n",
        "    negative += 1 \n",
        "\n",
        "# Rrint the amount of positive reviews\n",
        "print(\"Positive review:\", positive) \n",
        "\n",
        "# print the amount of negative reviews\n",
        "print(\"Negative review:\", negative) \n",
        "\n",
        "# Print the data variable for checking purposes, whether or not the contents of the CSV file has been read and stored properly\n",
        "print(data) "
      ],
      "metadata": {
        "id": "N_62BbcyWwiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the Tokenizer's goal and requirements (including translating all words to lower cases, remove punctuations)\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=None,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower=True, \n",
        "    split=' ',\n",
        "    char_level=False,\n",
        "    oov_token=None,\n",
        "    analyzer=None) \n",
        "\n",
        "# assign each words with a unique numerical value (integers)\n",
        "tokenizer.fit_on_texts(data['review'].values) \n",
        "\n",
        "# convert each words into its corresponding integers\n",
        "X = tokenizer.texts_to_sequences(data['review'].values) \n",
        "\n",
        "# equalize the list's length of all text to the longest sequence in the list (review entry)\n",
        "X = pad_sequences(X) "
      ],
      "metadata": {
        "id": "HA6P46ut8M6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store the rating part of the data into variable Y\n",
        "Y = data['rating'] \n",
        "\n",
        "# split the dataset (through the X and Y variable) into two, namely X_train and Y_train for training purposes, while X_test and Y_test for testing purposes\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = np.random) \n",
        "\n",
        "# Print the size of the training dataset for checking purposes\n",
        "print(X_train.shape,Y_train.shape) \n",
        "\n",
        "# Print the size of the testing dataset for checking purposes\n",
        "print(X_test.shape,Y_test.shape) "
      ],
      "metadata": {
        "id": "6xqttYG8G5Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Selecting Sequential as the model in order to build the model layer-by-layer\n",
        "model = Sequential() \n",
        "\n",
        "#Add the embedding layer to the model \n",
        "model.add(Embedding(len(tokenizer.word_index)+1, 100, input_length = X.shape[1]))\n",
        "\n",
        "#Add the LSTM layer to the model\n",
        "model.add(LSTM(128))\n",
        "\n",
        "#Add the Dropout layer to the model\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Add the Dense layer to the model that utilizes sigmoid activation function\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#Compile all layers while implemeting binary cross entropy for the loss parameter, Adam as the optimer, and accuracy as the metrics\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(learning_rate = 0.001),  metrics = [\"accuracy\"])\n",
        "\n",
        "#Print the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "1HZ7So8yGp2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with the training dataset (X_train and Y_train) with 100 batch size and 5 epocs\n",
        "test = model.fit(X_train, Y_train, batch_size=100, validation_data = (X_test, Y_test), epochs=5)  "
      ],
      "metadata": {
        "id": "sF3i4dYrHQRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the model's training accuracy score as epoch increments\n",
        "plt.plot(test.history['accuracy'])\n",
        "\n",
        "#Plot the model's accuracy score when predicting unseen data (testing data) as epoch increments\n",
        "plt.plot(test.history['val_accuracy'])\n",
        "\n",
        "#Label the graph title\n",
        "plt.title('model accuracy')\n",
        "\n",
        "#Label the graph y-axis\n",
        "plt.ylabel('accuracy')\n",
        "\n",
        "#Label the graph X-axis\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "#Activate the label/legend function and place it in the upper left corner\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "\n",
        "#Show the graph\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Sp7RxxQuG6Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the model's training loss score as epoch increments\n",
        "plt.plot(test.history['loss'])\n",
        "\n",
        "#Plot the model's loss score when predicting unseen data (testing data) as epoch increments\n",
        "plt.plot(test.history['val_loss'])\n",
        "\n",
        "#Label the graph title\n",
        "plt.title('model loss')\n",
        "\n",
        "#Label the graph y-axis\n",
        "plt.ylabel('loss')\n",
        "\n",
        "#Label the graph x-axis\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "#Activate the label/legend function and place it in the upper right corner\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "\n",
        "#Show the graph\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "--6yFHSsHKj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a variable for true positive counter\n",
        "true_positive = 0\n",
        "\n",
        "# Set a variable for true negative counter\n",
        "true_negative = 0\n",
        "\n",
        "# Set a variable for false positive counter\n",
        "false_positive = 0\n",
        "\n",
        "# Set a variable for false negative counter\n",
        "false_negative = 0\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "  # Counts the amount of true positive outputs\n",
        "  if (model.predict(X_test[i].reshape(1,250)) > 0.5) and (Y_test.values[i] == 1.0):\n",
        "    true_positive += 1\n",
        "  # Counts the amount of true negative outputs\n",
        "  elif (model.predict(X_test[i].reshape(1,250)) < 0.5) and (Y_test.values[i] == 0.0):\n",
        "    true_negative += 1\n",
        "  # Counts the amount of false negative outputs\n",
        "  elif (model.predict(X_test[i].reshape(1,250)) < 0.5) and (Y_test.values[i] == 1.0):\n",
        "    false_negative += 1 \n",
        "  # Counts the amount of false positive outputs\n",
        "  elif (model.predict(X_test[i].reshape(1,250)) > 0.5) and (Y_test.values[i] == 0.0):\n",
        "    false_positive += 1 "
      ],
      "metadata": {
        "id": "xE4EiAlxOKfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the amount of the true positive outputs\n",
        "print(\"True positive  : \" , true_positive)\n",
        "\n",
        "# Print the amount of the true negative outputs\n",
        "print(\"True negative  : \" , true_negative)\n",
        "\n",
        "# Print the amount of the false positive outputs\n",
        "print(\"False positive : \" , false_positive)\n",
        "\n",
        "# Print the amount of the false negative outputs\n",
        "print(\"False negative : \" , false_negative)"
      ],
      "metadata": {
        "id": "nLNr9bioQSYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the accuracy score of the model\n",
        "accuracy = (true_positive + true_negative)/(true_positive + true_negative + false_positive + false_negative)\n",
        "\n",
        "#Calculate the precision score of the model\n",
        "precision = true_positive / (true_positive + false_positive)\n",
        "\n",
        "#Calculate the recall score of the model\n",
        "recall = true_positive / (true_positive + false_negative)\n",
        "\n",
        "#Calculate the F1 score of the model\n",
        "f1_score = 2*((precision*recall)/(precision+recall))\n",
        "\n",
        "#Print the name of the model\n",
        "print(\"Evaluation of Long Short-Term Memory for Sentiment Analysis:\")\n",
        "\n",
        "#Print the accuracy score of the model\n",
        "print(\"Accuracy   : %.4f\" %accuracy)\n",
        "\n",
        "#Print the precision score of the model\n",
        "print(\"Precision  : %.4f\" %precision)\n",
        "\n",
        "#Print the recall score of the model\n",
        "print(\"Recall     : %.4f\" %recall)\n",
        "\n",
        "#Print the F1 score of the model\n",
        "print(\"F1 Score   : %.4f\" %f1_score)"
      ],
      "metadata": {
        "id": "b8P6zFg8TPcM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}